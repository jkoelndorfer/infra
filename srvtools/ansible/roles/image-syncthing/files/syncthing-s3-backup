#!/bin/bash

# This script waits for the locally running instance of Syncthing
# to become fully synced, then performs a backup using Duplicity
# and shuts down the instance.

SYNCTHING_POLL_TIME=30
# Path to Syncthing's config.xml.
if [[ "$BACKUP_LOCALDEV" != '1' ]]; then
    SYNCTHING_CONFIG_PATH='/srv/data/syncthing/config/config.xml'
else
    SYNCTHING_CONFIG_PATH="$HOME/.config/syncthing/config.xml"
fi
syncthing_api_key="$(xmllint "$SYNCTHING_CONFIG_PATH" --xpath "configuration/gui/apikey/text()")"

function s() {
    local suburl="$1"
    shift
    curl -s -H "X-API-Key: $syncthing_api_key" "$@" "http://127.0.0.1:8384/rest/$suburl"
}

function list_folder_ids() {
    s system/config | jq -r '.folders[].id'
}

function folder_sync_status() {
    local folder_id="$1"
    # paste joins all the lines together.
    local folder_status="$(s db/status -G --data-urlencode "folder=$folder_id" |
        jq -r '.needBytes,.needDeletes,.needDirectories,.needFiles,.needSymlinks' | paste -sd ' ')"

    # We'd expect each "need" value to be 0. If everything is in sync, status should be:
    #
    # 0 0 0 0 0

    [[ "$folder_status" == '0 0 0 0 0' ]]
}

function device_stats() {
    jq -s '.[1] as $devstats | .[0] | map(.[] | {(.id): {name, last_seen: $devstats[.id]["lastSeen"]}}) | add' \
        <(s system/config | jq '.devices | map({(.deviceID): { id: .deviceID, name: .name }})') \
        <(s stats/device)
}

function device_last_seen() {
    local device_id="$1"
    local last_seen_date=$(device_stats | jq -r '."'"$device_id"'".last_seen')
    if [[ -z "$last_seen_date" ]]; then
        return 1
    fi
    date -d "$last_seen_date" '+%s'
}

function folder_stats() {
    jq -s '.[1] as $fstats | .[0].folders | map({id: .id, label: .label, path: .path, last_activity: $fstats[(.id)]["lastFile"]["at"]})' \
        <(s system/config) <(s stats/folder)
}

function log() {
    msg="$1"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $msg" >&2
}

function get_backup_device_id() {
    if [[ "$BACKUP_LOCALDEV" == 1 ]]; then
        echo "$BACKUP_DEVICE_ID"
    else
        /opt/srvtools/aws/ssm-param --name "/$INSTANCE_ENV/backup/syncthing_backup_device_id" --region "$instance_region"
    fi
}

function main() {
    source /etc/metadata
    local instance_region="$(/opt/srvtools/aws/instance/region)"

    local backup_date="$(date '+%Y-%m-%d')"
    local backup_s3_bucket="$(/opt/srvtools/aws/ssm-param --name "/$INSTANCE_ENV/backup/backup_s3_bucket" --region "$instance_region")"
    if [[ -z "$backup_s3_bucket" ]]; then
        log 'Failed getting backup S3 bucket name!'
    else
        backup_run_report="$(backup "$instance_region" "$backup_s3_bucket" 2>&1)"
    fi

    local backup_report_topic_arn="$(/opt/srvtools/aws/ssm-param --name "/$INSTANCE_ENV/backup/backup_sns_topic_arn" --region "$instance_region")"
    local report=$(backup_report "$backup_s3_bucket" "$backup_run_report")
    aws sns publish \
        --topic-arn "$backup_report_topic_arn" \
        --subject "Backup Report $backup_date" \
        --message "$report" \
        --region "$instance_region"

    local do_scale_down="$(/opt/srvtools/aws/ssm-param --name "/$INSTANCE_ENV/backup/scale_down_after_backup" --region "$instance_region")"
    if [[ "$do_scale_down" == '1' && "$BACKUP_LOCALDEV" != '1' ]]; then
        sync
        mount -o remount,ro /srv/data
        sync
        aws autoscaling update-auto-scaling-group --auto-scaling-group-name "$INSTANCE_ASG_NAME" --desired-capacity 0 --region "$instance_region"
    fi
}

function backup_report() {
    local backup_s3_bucket="$1"
    local backup_run_report="$2"
    local updated_files="$(find /srv/data/syncthing -type f -mmin -$((60 * 24)) | grep -v -e '/.git/')"
    if [[ -z "$updated_files" ]]; then
        local updated_files='[none]'
    fi
cat <<EOF
################################################################################
#                                 UPDATED FILES                                #
################################################################################
$updated_files

################################################################################
#                                  BACKUP RUN                                  #
################################################################################
$backup_run_report

################################################################################
#                                  DISK USAGE                                  #
################################################################################
$(df -h /srv/data)

################################################################################
#                                  S3 BUCKET                                   #
################################################################################
$(aws s3 ls --human-readable "s3://$backup_s3_bucket" | sort -r)
EOF
}

function wait_for_backup_device() {
    local instance_region="$1"
    local start_date="$2"
    local backup_device_id="$(get_backup_device_id)"

    if [[ -n "$backup_device_id" ]]; then
        local backup_device_last_seen_time=0
        log "Got backup device ID $backup_device_id; waiting for device communication"
        while true; do
            backup_device_last_seen_time="$(device_last_seen "$backup_device_id")"
            if [[ -n "$backup_device_last_seen_time" ]]; then
                if [[ "$backup_device_last_seen_time" -gt "$start_date" ]]; then
                    log "Backup device $backup_device_id last seen time up-to-date now; continuing"
                    sleep 5
                    break
                fi
            else
                # Could not get the last seen time for the backup device for some reason.
                # This should not happen unless Syncthing glitches out.
                log 'BUG: Could not get backup device last seen time from Syncthing! This should not happen.'

                # If we end up here somehow, let's just wait a little while and try again.
                sleep 5
            fi
        done
    else
        log 'Failed to determine backup device ID; falling back to simple wait.'
        sleep 300
    fi
}

function wait_for_folders_synced() {
    local synced=0
    local total_poll_time=0
    while [[ "$synced" == 0 ]]; do
        # Check the folders after every iteration in case any new ones show up.
        local folders=($(list_folder_ids))
        local synced=1
        for f in "${folders[@]}"; do
            if ! folder_sync_status "$f"; then
                synced=0
                log "Folder $f is not synced" >&2
            else
                log "Folder $f is synced" >&2
            fi
        done
        if [[ "$synced" == 0 ]]; then
            log "Not all folders are synced; rechecking in $SYNCTHING_POLL_TIME seconds"
            local total_poll_time="$((total_poll_time + $SYNCTHING_POLL_TIME))"
            sleep "$SYNCTHING_POLL_TIME"
        fi
    done
    log "Synced after $total_poll_time seconds"
}

function backup() {
    local aws_region="$1"
    local backup_s3_bucket="$2"
    local start_date="$(date +%s)"

    log 'starting syncthing container'
    log "$(docker start syncthing 2>&1)"
    log 'waiting for syncthing to start up'
    sleep 30
    wait_for_backup_device "$aws_region" "$start_date"
    wait_for_folders_synced

    # Now that we're all up-to-date, stop Syncthing so there aren't any further
    # changes while our backup runs.
    log 'stopping syncthing container'
    log "$(docker stop syncthing 2>&1)"

    log
    log 'starting duplicity backup'
    local duplicity_remote="s3://s3.amazonaws.com/$backup_s3_bucket"
    duplicity \
        --archive-dir /srv/data/duplicity-archive \
        --full-if-older-than 7D \
        --no-encryption \
        --s3-use-multiprocessing \
        /srv/data/syncthing \
        "$duplicity_remote" 2>&1

    log 'pruning old backups'
    duplicity \
        remove-older-than 60D \
        --force \
        "$duplicity_remote" 2>&1
}

if [[ "$SHELL" == '/bin/bash' && "${BASH_SOURCE[0]}" == "$0" ]]; then
    main
fi
